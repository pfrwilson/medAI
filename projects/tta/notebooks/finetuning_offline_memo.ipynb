{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fs01/home/abbasgln/codes/medAI/projects/tta\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Loading environment variables\n",
    "load_dotenv()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import typing as tp\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "import medAI\n",
    "from medAI.utils.setup import BasicExperiment, BasicExperimentConfig\n",
    "\n",
    "from utils.metrics import MetricCalculator\n",
    "\n",
    "from timm.optim.optim_factory import create_optimizer\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "from datasets.datasets import ExactNCT2013RFImagePatches\n",
    "from medAI.datasets.nct2013 import (\n",
    "    KFoldCohortSelectionOptions,\n",
    "    LeaveOneCenterOutCohortSelectionOptions, \n",
    "    PatchOptions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAVE_OUT='CRCEO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing positions: 100%|██████████| 622/622 [00:08<00:00, 72.69it/s] \n",
      "Computing positions: 100%|██████████| 991/991 [00:17<00:00, 56.19it/s]\n",
      "Computing positions: 100%|██████████| 1469/1469 [00:26<00:00, 55.75it/s]\n"
     ]
    }
   ],
   "source": [
    "###### No support dataset ######\n",
    "\n",
    "from vicreg_pretrain_experiment import PretrainConfig\n",
    "config = PretrainConfig(cohort_selection_config=LeaveOneCenterOutCohortSelectionOptions(leave_out=f\"{LEAVE_OUT}\"))\n",
    "\n",
    "from baseline_experiment import BaselineConfig\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.tv_tensors import Image as TVImage\n",
    "\n",
    "class Transform:\n",
    "    def __init__(selfT, augment=False):\n",
    "        selfT.augment = augment\n",
    "        selfT.size = (256, 256)\n",
    "        # Augmentation\n",
    "        selfT.transform = T.Compose([\n",
    "            T.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
    "            T.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0.5),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "        ])  \n",
    "    \n",
    "    def __call__(selfT, item):\n",
    "        patch = item.pop(\"patch\")\n",
    "        patch = copy(patch)\n",
    "        patch = (patch - patch.min()) / (patch.max() - patch.min()) \\\n",
    "            if config.instance_norm else patch\n",
    "        patch = TVImage(patch)\n",
    "        patch = T.Resize(selfT.size, antialias=True)(patch).float()\n",
    "        \n",
    "        label = torch.tensor(item[\"grade\"] != \"Benign\").long()\n",
    "        \n",
    "        if selfT.augment:\n",
    "            patch_augs = torch.stack([selfT.transform(patch) for _ in range(2)], dim=0)\n",
    "            return patch_augs, patch, label, item\n",
    "        \n",
    "        return -1, patch, label, item\n",
    "\n",
    "\n",
    "cohort_selection_options_train = copy(config.cohort_selection_config)\n",
    "cohort_selection_options_train.min_involvement = config.min_involvement_train\n",
    "cohort_selection_options_train.benign_to_cancer_ratio = config.benign_to_cancer_ratio_train\n",
    "cohort_selection_options_train.remove_benign_from_positive_patients = config.remove_benign_from_positive_patients_train\n",
    "\n",
    "train_ds = ExactNCT2013RFImagePatches(\n",
    "    split=\"train\",\n",
    "    transform=Transform(augment=False),\n",
    "    cohort_selection_options=cohort_selection_options_train,\n",
    "    patch_options=config.patch_config,\n",
    "    debug=config.debug,\n",
    ")\n",
    "\n",
    "val_ds = ExactNCT2013RFImagePatches(\n",
    "    split=\"val\",\n",
    "    transform=Transform(augment=True),\n",
    "    cohort_selection_options=config.cohort_selection_config,\n",
    "    patch_options=config.patch_config,\n",
    "    debug=config.debug,\n",
    ")\n",
    "\n",
    "test_ds = ExactNCT2013RFImagePatches(\n",
    "    split=\"test\",\n",
    "    transform=Transform(augment=True),\n",
    "    cohort_selection_options=config.cohort_selection_config,\n",
    "    patch_options=config.patch_config,\n",
    "    debug=config.debug,\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=config.batch_size, shuffle=True, num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=config.batch_size, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=config.batch_size, shuffle=False, num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vicreg_pretrain_experiment import TimmFeatureExtractorWrapper\n",
    "from timm.layers.adaptive_avgmax_pool import SelectAdaptivePool2d\n",
    "\n",
    "\n",
    "fe_config = config.model_config\n",
    "\n",
    "# Create the model\n",
    "model: nn.Module = timm.create_model(\n",
    "    fe_config.model_name,\n",
    "    num_classes=fe_config.num_classes,\n",
    "    in_chans=1,\n",
    "    features_only=fe_config.features_only,\n",
    "    norm_layer=lambda channels: nn.GroupNorm(\n",
    "                    num_groups=fe_config.num_groups,\n",
    "                    num_channels=channels\n",
    "                    ))\n",
    "\n",
    "# Separate creation of classifier and global pool from feature extractor\n",
    "global_pool = SelectAdaptivePool2d(\n",
    "    pool_type='avg',\n",
    "    flatten=True,\n",
    "    input_fmt='NCHW',\n",
    "    )\n",
    "\n",
    "model = nn.Sequential(TimmFeatureExtractorWrapper(model), global_pool)\n",
    "\n",
    "\n",
    "# CHECkPOINT_PATH = os.path.join(os.getcwd(), f'logs/tta/vicreg_pretrain_gn_loco/vicreg_pretrain_gn_loco_{LEAVE_OUT}/', 'best_model.ckpt')\n",
    "# CHECkPOINT_PATH = os.path.join(os.getcwd(), f'logs/tta/vicreg_pretrn_5e-3-20linprob_gn_loco/vicreg_pretrn_5e-3-20linprob_gn_loco_{LEAVE_OUT}/', 'best_model.ckpt')\n",
    "CHECkPOINT_PATH = os.path.join(os.getcwd(), f'logs/tta/vicreg_pretrn_2048zdim_gn_loco/vicreg_pretrn_2048zdim_gn_loco_{LEAVE_OUT}/', 'best_model.ckpt')\n",
    "\n",
    "model.load_state_dict(torch.load(CHECkPOINT_PATH)['model'])\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "a = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train uisng finetuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 0/776 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 776/776 [00:56<00:00, 13.78it/s]\n",
      "train: 100%|██████████| 776/776 [00:45<00:00, 16.89it/s]\n",
      "train: 100%|██████████| 776/776 [00:46<00:00, 16.64it/s]\n",
      "train: 100%|██████████| 776/776 [00:46<00:00, 16.72it/s]\n",
      "train: 100%|██████████| 776/776 [00:47<00:00, 16.27it/s]\n",
      "train: 100%|██████████| 776/776 [00:46<00:00, 16.70it/s]\n",
      "train: 100%|██████████| 776/776 [00:46<00:00, 16.54it/s]\n",
      "train: 100%|██████████| 776/776 [00:46<00:00, 16.56it/s]\n",
      "train: 100%|██████████| 776/776 [00:50<00:00, 15.39it/s]\n",
      "train: 100%|██████████| 776/776 [00:45<00:00, 16.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from models.finetuner import Fineturner\n",
    "\n",
    "metric_calculator = MetricCalculator()\n",
    "finetuner_model: Fineturner = Fineturner(model, 512, 2, metric_calculator=metric_calculator, log_wandb=False)\n",
    "finetuner_model.train(train_loader,\n",
    "                  epochs=10,\n",
    "                  train_backbone=True,\n",
    "                  lr=1e-3\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 3011/3011 [06:55<00:00,  7.24it/s]\n"
     ]
    }
   ],
   "source": [
    "finetuner_model.metric_calculator.reset()\n",
    "desc='test'\n",
    "finetuner_model.validate(test_loader, desc=desc)\n",
    "metric_calculator = finetuner_model.metric_calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train linear model on reprs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train reprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6531977331904f539cc87ef988cac9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models.linear_prob import LinearProb\n",
    "\n",
    "loader = train_loader\n",
    "\n",
    "desc = \"train\"\n",
    "metric_calculator = MetricCalculator()\n",
    "# linear_prob = nn.Linear(512, 2).cuda()\n",
    "# optimizer = optim.Adam(linear_prob.parameters(), lr=1e-4)\n",
    "all_reprs_labels_metadata_train = []\n",
    "all_reprs = []\n",
    "all_labels = []\n",
    "for i, batch in enumerate(tqdm(loader, desc=desc)):\n",
    "    batch = deepcopy(batch)\n",
    "    images_augs, images, labels, meta_data = batch\n",
    "    images_augs = images_augs.cuda()\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    \n",
    "    reprs = model(images).detach()\n",
    "    all_reprs.append(reprs.cpu().numpy())\n",
    "    all_labels.append(labels.cpu().numpy())\n",
    "    all_reprs_labels_metadata_train.append((reprs, labels, meta_data))\n",
    "\n",
    "    # logits = linear_prob(reprs)\n",
    "    # loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "    \n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "all_reprs = np.concatenate(all_reprs, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKlearn logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LR = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "# LR.fit(all_reprs, all_labels)\n",
    "\n",
    "# # Assuming your input features have the same dimension as the scikit-learn model\n",
    "# input_features = LR.coef_.shape[1]  # Replace with the actual number of features\n",
    "# linear_prob = nn.Linear(input_features, 1) # Binary classification (1 output unit)\n",
    "\n",
    "# # Step 4: Assign the weights and bias from scikit-learn model to PyTorch model\n",
    "# with torch.no_grad():  # Disable gradient computation for this operation\n",
    "#     linear_prob.weight.data = torch.from_numpy(LR.coef_).float()\n",
    "#     linear_prob.bias.data = torch.from_numpy(LR.intercept_).float()\n",
    "\n",
    "# linear_prob.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_linear_prob: 100%|██████████| 932/932 [00:21<00:00, 42.44it/s] \n",
      "train_linear_prob: 100%|██████████| 932/932 [00:01<00:00, 572.63it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:02<00:00, 445.22it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:01<00:00, 597.71it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:02<00:00, 450.82it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:01<00:00, 595.35it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:01<00:00, 537.73it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:02<00:00, 431.97it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:01<00:00, 647.99it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:01<00:00, 601.65it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:02<00:00, 464.58it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:01<00:00, 548.07it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:01<00:00, 668.93it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:01<00:00, 666.74it/s]\n",
      "train_linear_prob: 100%|██████████| 932/932 [00:02<00:00, 409.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "linear_prob: LinearProb = LinearProb(512, 2, metric_calculator=metric_calculator, log_wandb=False)\n",
    "linear_prob.train(all_reprs_labels_metadata_train,\n",
    "                  epochs=15,\n",
    "                  lr=5e-3\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEMO on finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9933cde6d34ccfa8d0e38b1c37f5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/1715 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = test_loader\n",
    "adapt_to_test = True\n",
    "\n",
    "from memo_experiment import batched_marginal_entropy\n",
    "metric_calculator = MetricCalculator()\n",
    "desc = \"test\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for i, batch in enumerate(tqdm(loader, desc=desc)):\n",
    "    batch = deepcopy(batch)\n",
    "    images_augs, images, labels, meta_data = batch\n",
    "    images_augs = images_augs.cuda()\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    \n",
    "    batch_size, aug_size= images_augs.shape[0], images_augs.shape[1]\n",
    "\n",
    "    adaptation_fe_model = deepcopy(finetuner_model.feature_extractor)\n",
    "    adaptation_head_model = deepcopy(finetuner_model.linear)\n",
    "    # adaptation_head_model = deepcopy(linear_prob.linear)\n",
    "    if adapt_to_test:\n",
    "        # Adapt to test\n",
    "        _images_augs = images_augs.reshape(-1, *images_augs.shape[2:]).cuda()\n",
    "        # adaptation_head_model = deepcopy(linear_prob)\n",
    "        # adaptation_fe_model.eval()\n",
    "        params = [{\"params\": adaptation_head_model.parameters()}, {\"params\": adaptation_fe_model.parameters()}]\n",
    "        optimizer = optim.SGD(params, lr=1e-3)\n",
    "        \n",
    "        # optimizer = optim.SGD(adaptation_head_model.parameters(), lr=1e-10)\n",
    "        # reprs = adaptation_fe_model(_images_augs).detach() # for only adapting head\n",
    "        for j in range(1):\n",
    "            optimizer.zero_grad()\n",
    "            reprs = adaptation_fe_model(_images_augs) # for only adapting head\n",
    "            outputs = adaptation_head_model(reprs).reshape(batch_size, aug_size, -1)  \n",
    "            loss, logits = batched_marginal_entropy(outputs)\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    \n",
    "    reprs = adaptation_fe_model(images)\n",
    "    logits = adaptation_head_model(reprs)\n",
    "    loss = criterion(logits, labels)\n",
    "                    \n",
    "    # Update metrics   \n",
    "    metric_calculator.update(\n",
    "        batch_meta_data = meta_data,\n",
    "        probs = nn.functional.softmax(logits, dim=-1).detach().cpu(),\n",
    "        # probs = nn.functional.tanh(logits).detach().cpu(),\n",
    "        labels = labels.detach().cpu(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test/patch_auroc': tensor(0.6951),\n",
       " 'test/patch_accuracy': tensor(0.7420),\n",
       " 'test/all_inv_patch_auroc': tensor(0.6334),\n",
       " 'test/all_inv_patch_accuracy': tensor(0.7169),\n",
       " 'test/core_auroc': tensor(0.7973),\n",
       " 'test/core_accuracy': tensor(0.8507),\n",
       " 'test/all_inv_core_auroc': tensor(0.7069),\n",
       " 'test/all_inv_core_accuracy': tensor(0.8137)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log metrics every epoch\n",
    "metrics = metric_calculator.get_metrics()\n",
    "\n",
    "# Update best score\n",
    "(\n",
    "    best_score_updated,\n",
    "    best_score\n",
    "    ) = metric_calculator.update_best_score(metrics, desc)\n",
    "\n",
    "best_score_updated = copy(best_score_updated)\n",
    "best_score = copy(best_score)\n",
    "        \n",
    "# Log metrics\n",
    "metrics_dict = {\n",
    "    f\"{desc}/{key}\": value for key, value in metrics.items()\n",
    "    }\n",
    "metrics_dict.update(best_score) if desc == \"val\" else None \n",
    "\n",
    "\n",
    "# wandb.log(\n",
    "#     metrics_dict,\n",
    "#     )\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log with wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/fs01/home/abbasgln/codes/medAI/projects/wandb/run-20240123_144639-gisey84m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahdigilany/tta/runs/gisey84m' target=\"_blank\">offline_memo_vicreg_whl-fintn_10ep_2048zdim_gn_loco_CRCEO</a></strong> to <a href='https://wandb.ai/mahdigilany/tta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahdigilany/tta' target=\"_blank\">https://wandb.ai/mahdigilany/tta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahdigilany/tta/runs/gisey84m' target=\"_blank\">https://wandb.ai/mahdigilany/tta/runs/gisey84m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mahdigilany/tta/runs/gisey84m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9919ec26e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "group=f\"offline_memo_vicreg_whl-fintn_10ep_2048zdim_gn_loco\"\n",
    "name= group + f\"_{LEAVE_OUT}\"\n",
    "wandb.init(project=\"tta\", entity=\"mahdigilany\", name=name, group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa2ca33dd80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/h/abbasgln/.conda/envs/mttt/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/h/abbasgln/.conda/envs/mttt/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/h/abbasgln/.conda/envs/mttt/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/h/abbasgln/.conda/envs/mttt/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/h/abbasgln/.conda/envs/mttt/lib/python3.10/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/h/abbasgln/.conda/envs/mttt/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files: '/fs01/home/abbasgln/codes/medAI/projects'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m metrics_dict\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m      4\u001b[0m     metrics_dict,\n\u001b[1;32m      5\u001b[0m     )\n\u001b[0;32m----> 6\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mttt/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:4013\u001b[0m, in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   4003\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[1;32m   4004\u001b[0m \n\u001b[1;32m   4005\u001b[0m \u001b[38;5;124;03mThis is used when creating multiple runs in the same process.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4010\u001b[0m \u001b[38;5;124;03m    quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   4011\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mrun:\n\u001b[0;32m-> 4013\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mttt/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:420\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mttt/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:361\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mttt/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1953\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_noop\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_attach\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinish\u001b[39m(\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;28mself\u001b[39m, exit_code: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, quiet: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1944\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[1;32m   1945\u001b[0m \n\u001b[1;32m   1946\u001b[0m \u001b[38;5;124;03m    This is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;124;03m        quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mttt/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1966\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown_hooks:\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m==\u001b[39m TeardownStage\u001b[38;5;241m.\u001b[39mEARLY:\n\u001b[0;32m-> 1966\u001b[0m         \u001b[43mhook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atexit_cleanup(exit_code\u001b[38;5;241m=\u001b[39mexit_code)\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl\u001b[38;5;241m.\u001b[39m_global_run_stack) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/mttt/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:447\u001b[0m, in \u001b[0;36m_WandbInit._jupyter_teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook\n\u001b[1;32m    446\u001b[0m ipython \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook\u001b[38;5;241m.\u001b[39mshell\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook\u001b[38;5;241m.\u001b[39msave_ipynb():\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mttt/lib/python3.10/site-packages/wandb/jupyter.py:435\u001b[0m, in \u001b[0;36mNotebook.save_history\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This saves all cell executions in the current session as a new notebook.\"\"\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnbformat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v4, validator, write\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun pip install nbformat to save notebook history\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1548\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1591\u001b[0m, in \u001b[0;36m_fill_cache\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 24] Too many open files: '/fs01/home/abbasgln/codes/medAI/projects'"
     ]
    }
   ],
   "source": [
    "# os.environ[\"WANDB_MODE\"] = \"enabled\"\n",
    "metrics_dict.update({\"epoch\": 0})\n",
    "wandb.log(\n",
    "    metrics_dict,\n",
    "    )\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mttt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
